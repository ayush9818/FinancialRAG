{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['convfinqa_queries.jsonl', 'ConvFinQA_qrels.tsv', 'FinanceBench_qrels.tsv', 'FinDER_qrels.tsv', 'TATQA_qrels.tsv', 'finqabench_corpus.jsonl', 'finder_corpus.jsonl', 'tatqa_corpus.jsonl', 'tatqa_queries.jsonl', 'multiheirtt_corpus.jsonl', 'convfinqa_corpus.jsonl', 'finqa_queries.jsonl', 'multiheirtt_queries.jsonl', 'finqa_corpus.jsonl', 'financebench_queries.jsonl', 'FinQA_qrels.tsv', 'FinQABench_qrels.tsv', 'sample_submission_.csv', 'MultiHeirtt_qrels.tsv', 'finder_queries.jsonl', 'financebench_corpus.jsonl', 'finqabench_queries.jsonl']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "#from dotenv import load_dotenv\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "import faiss\n",
    "from huggingface_hub import login\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.schema import MetadataMode, NodeWithScore, QueryBundle\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from tasks.eval_rag import evaluate_rag\n",
    "\n",
    "data_dir = Path.cwd().parent / 'financerag/icaif-24-finance-rag-challenge'\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "finq_bench_corpus_path = data_dir / 'convfinqa_corpus.jsonl/corpus.jsonl'\n",
    "finq_bench_query_path = data_dir / 'convfinqa_queries.jsonl/queries.jsonl'\n",
    "finq_bench_tsv_path = data_dir / 'ConvFinQA_qrels.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:FinQ Bench\n",
      "Total Corpus:2066\n",
      "Total Queries:421\n"
     ]
    }
   ],
   "source": [
    "finq_bench_corpus = pd.read_json(finq_bench_corpus_path, lines=True)\n",
    "finq_bench_queries = pd.read_json(finq_bench_query_path, lines=True)\n",
    "finq_bench_gt = pd.read_csv(finq_bench_tsv_path, sep='\\t')\n",
    "print(\"Dataset:FinQ Bench\\nTotal Corpus:{}\\nTotal Queries:{}\".format(finq_bench_corpus.shape[0], finq_bench_queries.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'title', 'text'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finq_bench_corpus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from llama_index.core import Document\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process documents from DataFrame\n",
    "def create_documents(df):\n",
    "    \"\"\"Create Documents with metadata from df\"\"\"\n",
    "    documents = []\n",
    "    for idx,row in df.iterrows():\n",
    "        _ = Document(\n",
    "            text=row['text'], \n",
    "            metadata={'_id' : row['_id'], 'title' : row['title']}\n",
    "            )\n",
    "        documents.append(_)\n",
    "    return documents\n",
    "\n",
    "# Load data from DataFrame\n",
    "documents = create_documents(finq_bench_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase the text for uniformity\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace multiple spaces/newlines with a single space/newline\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters and numeric encodings like \"2019s\"\n",
    "    text = re.sub(r'\\d{4}s?', '', text)  # Remove specific year encodings (e.g., 2019s)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.,]', '', text)  # Keep only alphanumeric chars, spaces, periods, and commas\n",
    "\n",
    "    # Remove isolated numbers and percentages\n",
    "    text = re.sub(r'\\b\\d+(\\.\\d+)?%\\b', '', text)  # Remove percentages like \"12.3%\"\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)  # Remove isolated numbers\n",
    "\n",
    "    # Optional: Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    return text\n",
    "\n",
    "def process_table_to_text(table_str):\n",
    "    # Split the table into lines\n",
    "    lines = table_str.strip().split('\\n')\n",
    "    \n",
    "    # Remove any leading or trailing empty lines\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    # Remove separator lines (lines with dashes or pipes and dashes)\n",
    "    lines = [line for line in lines if not re.match(r'^[-\\s|]+$', line)]\n",
    "    \n",
    "    if not lines:\n",
    "     #   print(\"Warning: No content found in the table.\")\n",
    "        return []\n",
    "    \n",
    "    # Now, assume that the first line is the header\n",
    "    header_line = lines[0]\n",
    "    data_lines = lines[1:]\n",
    "    \n",
    "    # Check if data_lines is empty\n",
    "    if not data_lines:\n",
    "   #     print(\"Warning: No data lines found in the table.\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the header\n",
    "    header_cells = [cell.strip() for cell in re.split(r'\\||\\s{2,}', header_line) if cell.strip()]\n",
    "    \n",
    "    # Now check if the number of header cells is less than the number of data columns\n",
    "    # We'll need to add a placeholder for the missing row label header\n",
    "    first_data_line = data_lines[0]\n",
    "    data_cells = [cell.strip() for cell in re.split(r'\\||\\s{2,}', first_data_line) if cell.strip()]\n",
    "    \n",
    "    if len(header_cells) < len(data_cells):\n",
    "        header_cells = ['Row Label'] + header_cells  # Add placeholder for row labels\n",
    "    \n",
    "    # Now parse data lines\n",
    "    table_data = []\n",
    "    for line in data_lines:\n",
    "        cells = [cell.strip() for cell in re.split(r'\\||\\s{2,}', line) if cell.strip()]\n",
    "        if cells:\n",
    "            table_data.append(cells)\n",
    "    \n",
    "    if not table_data:\n",
    "    #    print(\"Warning: No valid data rows found after parsing.\")\n",
    "        return []\n",
    "    \n",
    "    # Now construct sentences\n",
    "    sentences = []\n",
    "    for row in table_data:\n",
    "        row_label = row[0]\n",
    "        for col_idx in range(1, len(row)):\n",
    "            if col_idx < len(header_cells):\n",
    "                col_header = header_cells[col_idx]\n",
    "            else:\n",
    "                col_header = f\"Column {col_idx}\"\n",
    "            value = row[col_idx]\n",
    "            # Clean up value (remove $ and spaces)\n",
    "            value_cleaned = value.replace('$', '').strip()\n",
    "            # Construct the sentence\n",
    "            sentence = f\"{col_header} {row_label} value is {value_cleaned}.\"\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def separate_and_clean_text(doc):\n",
    "    # Ensure the input is a Document object\n",
    "    if not isinstance(doc, Document):\n",
    "        raise TypeError(\"Input must be a Document object.\")\n",
    "    \n",
    "    # Extract the text content from the document\n",
    "    text = doc.text\n",
    "    \n",
    "    # Regex pattern to identify table-like rows (rows with pipes '|')\n",
    "    table_pattern = re.compile(r\"\\|\")\n",
    "    \n",
    "    lines = text.split(\"\\n\")\n",
    "    tables = []\n",
    "    regular_text = []\n",
    "    current_table = []\n",
    "    \n",
    "    # Loop through lines to separate tables and text\n",
    "    for line in lines:\n",
    "        if table_pattern.search(line):\n",
    "            current_table.append(line)\n",
    "        else:\n",
    "            # If a regular text line is found after a table block, save the table\n",
    "            if current_table:\n",
    "                tables.append(\"\\n\".join(current_table))\n",
    "                current_table = []  # Reset for the next table block\n",
    "                \n",
    "            # Add the line to regular text\n",
    "            regular_text.append(line)\n",
    "    \n",
    "    # If there's a remaining table block at the end, add it\n",
    "    if current_table:\n",
    "        tables.append(\"\\n\".join(current_table))\n",
    "    \n",
    "    # Join and clean regular text lines\n",
    "    text_content = \"\\n\".join(regular_text)\n",
    "    cleaned_text = clean_text(text_content)\n",
    "    \n",
    "    # Process tables and convert to text sentences\n",
    "    table_sentences = []\n",
    "    for idx, table in enumerate(tables):\n",
    "   #     print(f\"Processing table {idx + 1}:\")\n",
    "      #  print(table)\n",
    "        sentences = process_table_to_text(table)\n",
    "        if sentences:\n",
    "            table_sentences.extend(sentences)\n",
    "    #    else:\n",
    "   #         print(f\"No sentences generated from table {idx + 1}.\")\n",
    "\n",
    "    # Combine the cleaned text and table sentences\n",
    "    all_text = cleaned_text + \"\\n\" + \"\\n\".join(table_sentences)\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "# Assuming 'documents[2]' is your document, call the function:\n",
    "#all_text = separate_and_clean_text(documents[0])\n",
    "#print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Text for Document[2]:\n",
      " five year million revolving , multi currency , senior unsecured credit facility maturing november , senior credit facility . . million outstanding senior credit facility december , , availability . million . senior credit facility contains provisions increase line million . also available uncommitted credit facilities totaling . million . may use excess cash borrow senior credit facility , subject limits set board directors , repurchase additional common stock . billion program expires december , . approximately . million remains authorized future repurchases plan . management believes cash flows operations available borrowings senior credit facility sufficient meet expected working capital , capital expenditure debt service needs . investment opportunities arise , believe earnings , balance sheet cash flows allow us obtain additional capital , necessary . contractual obligations entered contracts various third parties normal course business require future payments . following table illustrates contractual obligations millions contractual obligations total thereafter . longterm income taxes payable . . . . longterm liabilities . . . . total contractual obligations . . . . . critical accounting estimates financial results affected selection application accounting policies methods . significant accounting policies require management judgment discussed . excess inventory instruments must determine balance sheet date much , , inventory may ultimately prove unsaleable unsaleable carrying cost . similarly , must also determine instruments hand put productive use remain undeployed result excess supply . reserves established effectively adjust inventory instruments net realizable value . determine appropriate level reserves , evaluate current stock levels relation historical expected patterns demand products instrument systems components . basis determination generally inventory instrument items categories except workinprogress inventory , recorded cost . obsolete discontinued items generally destroyed completely written . management evaluates need changes valuation reserves based market conditions , competitive offerings factors regular basis . income taxes income tax expense , deferred tax assets liabilities reserves unrecognized tax benefits reflect management best assessment estimated future taxes paid . subject income taxes u.s . numerous foreign jurisdictions . significant judgments estimates required determining consolidated income tax expense . estimate income tax expense income tax liabilities assets taxable jurisdiction . realization deferred tax assets taxable jurisdiction dependent ability generate future taxable income sufficient realize benefits . evaluate deferred tax assets ongoing basis provide valuation allowances determined 201cmore likely 201d deferred tax benefit realized . federal income taxes provided portion income foreign subsidiaries expected remitted u.s . calculation tax liabilities involves dealing uncertainties application complex tax laws regulations multitude jurisdictions across global operations . subject regulatory review audit virtually jurisdictions reviews audits may require extended periods time resolve . record income tax provisions based knowledge relevant facts circumstances , including existing tax laws , experience previous settlement agreements , status current examinations understanding tax authorities view certain relevant industry commercial matters . recognize tax liabilities accordance financial accounting standards board fasb guidance income taxes adjust liabilities judgment changes result evaluation new information previously available . due complexity uncertainties , ultimate resolution may result payment materially different current estimate tax liabilities . differences reflected increases decreases income tax expense period determined . commitments contingencies accruals product liability claims established assistance internal external legal counsel based current information historical settlement information claims , related legal fees claims incurred reported . use actuarial model assist management determining appropriate level accruals product liability claims . historical patterns claim loss development z e r h l n g , n c .\n",
      "total long-term debt value is 1127.6.\n",
      "2010 long-term debt value is 2013.\n",
      "2011 and 2012 long-term debt value is 128.8.\n",
      "2013 and 2014 long-term debt value is 2013.\n",
      "2015 and thereafter long-term debt value is 998.8.\n",
      "total interest payments value is 1095.6.\n",
      "2010 interest payments value is 53.7.\n",
      "2011 and 2012 interest payments value is 103.8.\n",
      "2013 and 2014 interest payments value is 103.8.\n",
      "2015 and thereafter interest payments value is 834.3.\n",
      "total operating leases value is 134.6.\n",
      "2010 operating leases value is 37.3.\n",
      "2011 and 2012 operating leases value is 47.6.\n",
      "2013 and 2014 operating leases value is 26.6.\n",
      "2015 and thereafter operating leases value is 23.1.\n",
      "total purchase obligations value is 33.0.\n",
      "2010 purchase obligations value is 27.8.\n",
      "2011 and 2012 purchase obligations value is 5.1.\n",
      "2013 and 2014 purchase obligations value is 0.1.\n",
      "2015 and thereafter purchase obligations value is 2013.\n",
      "total long-term income taxes payable value is 94.3.\n",
      "2010 long-term income taxes payable value is 2013.\n",
      "2011 and 2012 long-term income taxes payable value is 56.5.\n",
      "2013 and 2014 long-term income taxes payable value is 15.3.\n",
      "2015 and thereafter long-term income taxes payable value is 22.5.\n",
      "total other long-term liabilities value is 234.2.\n",
      "2010 other long-term liabilities value is 2013.\n",
      "2011 and 2012 other long-term liabilities value is 81.7.\n",
      "2013 and 2014 other long-term liabilities value is 26.2.\n",
      "2015 and thereafter other long-term liabilities value is 126.3.\n",
      "total total contractual obligations value is 2719.3.\n",
      "2010 total contractual obligations value is 118.8.\n",
      "2011 and 2012 total contractual obligations value is 423.5.\n",
      "2013 and 2014 total contractual obligations value is 172.0.\n",
      "2015 and thereafter total contractual obligations value is 2005.0.\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'documents' is a list of Document objects\n",
    "for i, doc in enumerate(documents):\n",
    "    # Apply the separate and clean function to each document\n",
    "    cleaned_combined_text = separate_and_clean_text(doc)\n",
    "    \n",
    "    # Update the document's text with the cleaned and combined text\n",
    "    documents[i].text = cleaned_combined_text\n",
    "\n",
    "# Example: Check the updated text of the first document\n",
    "print(\"Updated Text for Document[2]:\\n\", documents[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "   EMBED_DIMENSION =  1024\n",
    "   EMBED_MODEL = \"baconnier/Finance_embedding_large_en-V0.1\"\n",
    "   RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "   SIM_TOP_K = 50\n",
    "   RERANKER_TOP_N = 30\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# Llamaindex global settings for llm and embeddings\n",
    "Settings.llm = None\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=cfg.EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class RetrievalAgent:\n",
    "    def __init__(self, cfg, documents):\n",
    "        self.cfg = cfg \n",
    "        self.documents = documents \n",
    "\n",
    "        self.index , self.reranker = self.initialise_retrieval_components()\n",
    "\n",
    "    def initialise_retrieval_components(self):\n",
    "        # Create FaisVectorStore to store embeddings\n",
    "        fais_index = faiss.IndexFlatL2(self.cfg.EMBED_DIMENSION)\n",
    "        vector_store = FaissVectorStore(faiss_index=fais_index)\n",
    "        print(\"Vector Store Created\")\n",
    "\n",
    "        ## Can experiment with different transformations\n",
    "        base_pipeline = IngestionPipeline(\n",
    "            # chunk_size=256, chunk_overlap=20\n",
    "            transformations=[SentenceSplitter()],\n",
    "            vector_store=vector_store,\n",
    "            documents=self.documents\n",
    "        )\n",
    "        nodes = base_pipeline.run()\n",
    "\n",
    "        # Create vector index from base nodes\n",
    "        index = VectorStoreIndex(nodes)\n",
    "        print(\"Vector Index Initialised\")\n",
    "        \n",
    "        # Create Reranker\n",
    "        reranker = SentenceTransformerRerank(\n",
    "                    model=self.cfg.RERANKER_MODEL,\n",
    "                    top_n=self.cfg.RERANKER_TOP_N\n",
    "                )\n",
    "        print(\"Reranker Initialised\")\n",
    "        return index, reranker \n",
    "\n",
    "    def retrieve_nodes(self, query_str, with_reranker=True):\n",
    "        query_bundle = QueryBundle(query_str)\n",
    "        # configure retriever\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=self.index,\n",
    "            similarity_top_k=self.cfg.SIM_TOP_K\n",
    "        )\n",
    "        retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "        if with_reranker:    \n",
    "            retrieved_nodes = self.reranker.postprocess_nodes(\n",
    "                retrieved_nodes, query_bundle\n",
    "            )\n",
    "\n",
    "        return retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_nodes(nodes, extract_unique=True):\n",
    "    init_rows = []\n",
    "    for node in nodes:\n",
    "        tmp = {\n",
    "            \"score\" : node.score,\n",
    "            \"text\" : node.text,\n",
    "            \"corpus_id\" : node.metadata['_id']\n",
    "        }\n",
    "        init_rows.append(tmp)\n",
    "    tmp_df = pd.DataFrame(init_rows)\n",
    "\n",
    "    if not extract_unique:\n",
    "        return tmp_df \n",
    "    \n",
    "    final_rows = []\n",
    "    for corpus_id, corpus_df in tmp_df.groupby('corpus_id'):\n",
    "        max_score = corpus_df['score'].max()\n",
    "        text = corpus_df[corpus_df.score == max_score].text.tolist()[0]\n",
    "        final_rows.append({\n",
    "            \"corpus_id\" : corpus_id, \n",
    "            \"text\" : text, \n",
    "            \"score\" : max_score\n",
    "        })\n",
    "    df = pd.DataFrame(final_rows)\n",
    "    df = df.sort_values(by='score', ascending=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_on_dataset(cfg, documents, queries, gt, with_reranker=True):\n",
    "  \n",
    "     # Initialize Retrieval Agent \n",
    "    ret_agent = RetrievalAgent(cfg=cfg, documents=documents)\n",
    "\n",
    "    query_id_list = []\n",
    "    corpus_id_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for idx,row in queries.iterrows():\n",
    "        query_id = row['_id']\n",
    "        query_text = row['text']\n",
    "\n",
    "        nodes = ret_agent.retrieve_nodes(query_text, with_reranker=with_reranker)\n",
    "        # Extract top 10 unique corpus_id\n",
    "        node_df = create_df_from_nodes(nodes)[:10]\n",
    "\n",
    "        query_id_list.extend([query_id] * 10)\n",
    "        corpus_id_list.extend(node_df.corpus_id.tolist())\n",
    "        score_list.extend(node_df.score.tolist())\n",
    "\n",
    "\n",
    "    final_df = pd.DataFrame({\n",
    "        \"query_id\" : query_id_list, \n",
    "        \"corpus_id\" : corpus_id_list,\n",
    "        \"score\" : score_list\n",
    "    })\n",
    "\n",
    "    # Convert the TSV data into a dictionary format for evaluation\n",
    "    qrels_dict = gt.groupby('query_id').apply(lambda x: dict(zip(x['corpus_id'], x['score']))).to_dict()\n",
    "    results = final_df.groupby('query_id').apply(lambda x: dict(zip(x['corpus_id'], x['score']))).to_dict()\n",
    "    print(evaluate_rag(qrels_dict, results, [1, 5, 10]))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvFinQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Reranker Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Created\n"
     ]
    }
   ],
   "source": [
    "evaluate_on_dataset(cfg=cfg,\n",
    "                    documents=documents, \n",
    "                    queries=finq_bench_queries,\n",
    "                    gt=finq_bench_gt,\n",
    "                    with_reranker=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Reranker Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Created\n",
      "Vector Index Initialised\n",
      "Reranker Initialised\n",
      "({'NDCG@1': 0.73333, 'NDCG@5': 0.8293, 'NDCG@10': 0.8293}, {'MAP@1': 0.73333, 'MAP@5': 0.79556, 'MAP@10': 0.79556}, {'Recall@1': 0.73333, 'Recall@5': 0.93333, 'Recall@10': 0.93333}, {'P@1': 0.73333, 'P@5': 0.18667, 'P@10': 0.09333})\n"
     ]
    }
   ],
   "source": [
    "evaluate_on_dataset(cfg=cfg,\n",
    "                    corpus=finq_bench_corpus, \n",
    "                    queries=finq_bench_queries,\n",
    "                    gt=finq_bench_gt,\n",
    "                    with_reranker=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10 ('fin_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24178e9080b9111c74efca0fe0e59b8013241aff9e5d7780cefaf674cfabb6e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
