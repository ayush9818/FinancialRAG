{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /nfs/home/scg1143/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "import faiss\n",
    "from huggingface_hub import login\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "from llama_index.core.schema import  QueryBundle\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from finance_rag import evaluate_rag\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "login(os.environ['HF_TOKEN'])\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "   EMBED_DIMENSION =  1024\n",
    "   EMBED_MODEL = \"baconnier/Finance_embedding_large_en-V0.1\"\n",
    "   RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "   SIM_TOP_K = 50\n",
    "   RERANKER_TOP_N = 30\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# Llamaindex global settings for llm and embeddings\n",
    "Settings.llm = None\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=cfg.EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(df):\n",
    "    \"\"\"Create Documents with metadata from df\"\"\"\n",
    "    documents = []\n",
    "    for idx,row in df.iterrows():\n",
    "        _ = Document(\n",
    "            text=row['text'], \n",
    "            metadata={'_id' : row['_id'], 'title' : row['title']}\n",
    "            )\n",
    "        documents.append(_)\n",
    "    return documents\n",
    "\n",
    "# TODO: Create Custom Retriever Class after finalizing the experiment\n",
    "# https://docs.llamaindex.ai/en/stable/examples/query_engine/CustomRetrievers/\n",
    "\n",
    "\n",
    "class HybridRetrievalAgent:\n",
    "    def __init__(self, cfg, documents):\n",
    "        self.cfg = cfg \n",
    "        self.documents = documents \n",
    "\n",
    "        self.index , self.reranker = self.initialise_retrieval_components()\n",
    "\n",
    "    def initialise_retrieval_components(self):\n",
    "        # Create FaisVectorStore to store embeddings\n",
    "        fais_index = faiss.IndexFlatL2(self.cfg.EMBED_DIMENSION)\n",
    "        vector_store = FaissVectorStore(faiss_index=fais_index)\n",
    "        print(\"Vector Store Created\")\n",
    "\n",
    "        ## Can experiment with different transformations\n",
    "        base_pipeline = IngestionPipeline(\n",
    "            # chunk_size=256, chunk_overlap=20\n",
    "            transformations=[SentenceSplitter()],\n",
    "            vector_store=vector_store,\n",
    "            documents=self.documents\n",
    "        )\n",
    "        nodes = base_pipeline.run()\n",
    "\n",
    "        # Create vector index from base nodes\n",
    "        index = VectorStoreIndex(nodes)\n",
    "        print(\"Vector Index Initialised\")\n",
    "        \n",
    "        # Create Reranker\n",
    "        reranker = SentenceTransformerRerank(\n",
    "                    model=self.cfg.RERANKER_MODEL,\n",
    "                    top_n=self.cfg.RERANKER_TOP_N\n",
    "                )\n",
    "        print(\"Reranker Initialised\")\n",
    "        return index, reranker \n",
    "\n",
    "    def retrieve_nodes(self, query_str, with_reranker=True):\n",
    "        query_bundle = QueryBundle(query_str)\n",
    "        # configure retriever\n",
    "        retriever = QueryFusionRetriever(\n",
    "            [\n",
    "                self.index.as_retriever(similarity_top_k=self.cfg.SIM_TOP_K),\n",
    "                BM25Retriever.from_defaults(\n",
    "                    docstore=self.index.docstore, similarity_top_k=self.cfg.SIM_TOP_K\n",
    "                ),\n",
    "            ],\n",
    "            num_queries=1,\n",
    "            use_async=True,\n",
    "        )\n",
    "        retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "        if with_reranker:    \n",
    "            retrieved_nodes = self.reranker.postprocess_nodes(\n",
    "                retrieved_nodes, query_bundle\n",
    "            )\n",
    "\n",
    "        return retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_nodes(nodes, extract_unique=True):\n",
    "    init_rows = []\n",
    "    for node in nodes:\n",
    "        tmp = {\n",
    "            \"score\" : node.score,\n",
    "            \"text\" : node.text,\n",
    "            \"corpus_id\" : node.metadata['_id']\n",
    "        }\n",
    "        init_rows.append(tmp)\n",
    "    tmp_df = pd.DataFrame(init_rows)\n",
    "\n",
    "    if not extract_unique:\n",
    "        return tmp_df \n",
    "    \n",
    "    final_rows = []\n",
    "    for corpus_id, corpus_df in tmp_df.groupby('corpus_id'):\n",
    "        max_score = corpus_df['score'].max()\n",
    "        text = corpus_df[corpus_df.score == max_score].text.tolist()[0]\n",
    "        final_rows.append({\n",
    "            \"corpus_id\" : corpus_id, \n",
    "            \"text\" : text, \n",
    "            \"score\" : max_score\n",
    "        })\n",
    "    df = pd.DataFrame(final_rows)\n",
    "    df = df.sort_values(by='score', ascending=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_on_dataset(cfg, corpus, queries, gt, with_reranker=True):\n",
    "    # Create FinQ Bench Documents\n",
    "    documents = create_documents(corpus)\n",
    "\n",
    "    # Initialize Retrieval Agent \n",
    "    ret_agent = HybridRetrievalAgent(cfg=cfg, documents=documents)\n",
    "\n",
    "    query_id_list = []\n",
    "    corpus_id_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for idx,row in queries.iterrows():\n",
    "        query_id = row['_id']\n",
    "        query_text = row['text']\n",
    "\n",
    "        nodes = ret_agent.retrieve_nodes(query_text, with_reranker=with_reranker)\n",
    "        # Extract top 10 unique corpus_id\n",
    "        node_df = create_df_from_nodes(nodes)[:10]\n",
    "\n",
    "        query_id_list.extend([query_id] * 10)\n",
    "        corpus_id_list.extend(node_df.corpus_id.tolist())\n",
    "        score_list.extend(node_df.score.tolist())\n",
    "\n",
    "\n",
    "    final_df = pd.DataFrame({\n",
    "        \"query_id\" : query_id_list, \n",
    "        \"corpus_id\" : corpus_id_list,\n",
    "        \"score\" : score_list\n",
    "    })\n",
    "\n",
    "    # Convert the TSV data into a dictionary format for evaluation\n",
    "    qrels_dict = gt.groupby('query_id').apply(lambda x: dict(zip(x['corpus_id'], x['score']))).to_dict()\n",
    "    results = final_df.groupby('query_id').apply(lambda x: dict(zip(x['corpus_id'], x['score']))).to_dict()\n",
    "    print(evaluate_rag(qrels_dict, results, [1, 5, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinQABench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['convfinqa_queries.jsonl', 'ConvFinQA_qrels.tsv', 'FinanceBench_qrels.tsv', 'FinDER_qrels.tsv', 'TATQA_qrels.tsv', 'finqabench_corpus.jsonl', 'finder_corpus.jsonl', 'tatqa_corpus.jsonl', 'tatqa_queries.jsonl', 'multiheirtt_corpus.jsonl', 'convfinqa_corpus.jsonl', 'finqa_queries.jsonl', 'multiheirtt_queries.jsonl', 'finqa_corpus.jsonl', 'financebench_queries.jsonl', 'FinQA_qrels.tsv', 'FinQABench_qrels.tsv', 'sample_submission_.csv', 'MultiHeirtt_qrels.tsv', 'finder_queries.jsonl', 'financebench_corpus.jsonl', 'finqabench_queries.jsonl']\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path.cwd().parent / 'data'\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "finq_bench_corpus_path = data_dir / 'finqabench_corpus.jsonl/corpus.jsonl'\n",
    "finq_bench_query_path = data_dir / 'finqabench_queries.jsonl/queries.jsonl'\n",
    "finq_bench_tsv_path = data_dir / 'FinQABench_qrels.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:FinQ Bench\n",
      "Total Corpus:92\n",
      "Total Queries:100\n"
     ]
    }
   ],
   "source": [
    "finq_bench_corpus = pd.read_json(finq_bench_corpus_path, lines=True)\n",
    "finq_bench_queries = pd.read_json(finq_bench_query_path, lines=True)\n",
    "finq_bench_gt = pd.read_csv(finq_bench_tsv_path, sep='\\t')\n",
    "print(\"Dataset:FinQ Bench\\nTotal Corpus:{}\\nTotal Queries:{}\".format(finq_bench_corpus.shape[0], finq_bench_queries.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "   EMBED_DIMENSION =  1024\n",
    "   EMBED_MODEL = \"baconnier/Finance_embedding_large_en-V0.1\"\n",
    "   RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "   SIM_TOP_K = 50\n",
    "   RERANKER_TOP_N = 30\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Created\n",
      "Vector Index Initialised\n",
      "Reranker Initialised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <coroutine object run_async_tasks.<locals>._gather at 0x7f84f7ce2740>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n",
      "Exception ignored in: <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x7f84ff741fc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n",
      "Exception ignored in: <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x7f84ff7427a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate_on_dataset(cfg\u001b[39m=\u001b[39mcfg,\n\u001b[1;32m      2\u001b[0m                     corpus\u001b[39m=\u001b[39mfinq_bench_corpus, \n\u001b[1;32m      3\u001b[0m                     queries\u001b[39m=\u001b[39mfinq_bench_queries,\n\u001b[1;32m      4\u001b[0m                     gt\u001b[39m=\u001b[39mfinq_bench_gt,\n\u001b[1;32m      5\u001b[0m                     with_reranker\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[9], line 53\u001b[0m, in \u001b[0;36mevaluate_on_dataset\u001b[0;34m(cfg, corpus, queries, gt, with_reranker)\u001b[0m\n\u001b[1;32m     49\u001b[0m     corpus_id_list\u001b[39m.\u001b[39mextend(node_df\u001b[39m.\u001b[39mcorpus_id\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m     50\u001b[0m     score_list\u001b[39m.\u001b[39mextend(node_df\u001b[39m.\u001b[39mscore\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m---> 53\u001b[0m final_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[1;32m     54\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mquery_id\u001b[39m\u001b[39m\"\u001b[39m : query_id_list, \n\u001b[1;32m     55\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcorpus_id\u001b[39m\u001b[39m\"\u001b[39m : corpus_id_list,\n\u001b[1;32m     56\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m : score_list\n\u001b[1;32m     57\u001b[0m })\n\u001b[1;32m     59\u001b[0m \u001b[39m# Convert the TSV data into a dictionary format for evaluation\u001b[39;00m\n\u001b[1;32m     60\u001b[0m qrels_dict \u001b[39m=\u001b[39m gt\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mquery_id\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(x[\u001b[39m'\u001b[39m\u001b[39mcorpus_id\u001b[39m\u001b[39m'\u001b[39m], x[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m])))\u001b[39m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/.conda/envs/fin_env/lib/python3.11/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy, typ\u001b[39m=\u001b[39mmanager)\n\u001b[1;32m    779\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/.conda/envs/fin_env/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39mdtype, typ\u001b[39m=\u001b[39mtyp, consolidate\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/.conda/envs/fin_env/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/.conda/envs/fin_env/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "evaluate_on_dataset(cfg=cfg,\n",
    "                    corpus=finq_bench_corpus, \n",
    "                    queries=finq_bench_queries,\n",
    "                    gt=finq_bench_gt,\n",
    "                    with_reranker=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10 ('fin_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24178e9080b9111c74efca0fe0e59b8013241aff9e5d7780cefaf674cfabb6e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
