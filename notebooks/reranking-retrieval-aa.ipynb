{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /nfs/home/scg1143/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "import faiss\n",
    "from huggingface_hub import login\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.schema import MetadataMode, NodeWithScore, QueryBundle\n",
    "\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from finance_rag import evaluate_rag\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "login(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "   EMBED_DIMENSION =  1024\n",
    "   EMBED_MODEL = \"baconnier/Finance_embedding_large_en-V0.1\"\n",
    "   RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "   SIM_TOP_K = 50\n",
    "   RERANKER_TOP_N = 30\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# Llamaindex global settings for llm and embeddings\n",
    "Settings.llm = None\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=cfg.EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinQABench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['convfinqa_queries.jsonl', 'ConvFinQA_qrels.tsv', 'FinanceBench_qrels.tsv', 'FinDER_qrels.tsv', 'TATQA_qrels.tsv', 'finqabench_corpus.jsonl', 'finder_corpus.jsonl', 'tatqa_corpus.jsonl', 'tatqa_queries.jsonl', 'multiheirtt_corpus.jsonl', 'convfinqa_corpus.jsonl', 'finqa_queries.jsonl', 'multiheirtt_queries.jsonl', 'finqa_corpus.jsonl', 'financebench_queries.jsonl', 'FinQA_qrels.tsv', 'FinQABench_qrels.tsv', 'sample_submission_.csv', 'MultiHeirtt_qrels.tsv', 'finder_queries.jsonl', 'financebench_corpus.jsonl', 'finqabench_queries.jsonl']\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path.cwd().parent / 'data'\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "finq_bench_corpus_path = data_dir / 'finqabench_corpus.jsonl/corpus.jsonl'\n",
    "finq_bench_query_path = data_dir / 'finqabench_queries.jsonl/queries.jsonl'\n",
    "finq_bench_tsv_path = data_dir / 'FinQABench_qrels.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:FinQ Bench\n",
      "Total Corpus:92\n",
      "Total Queries:100\n"
     ]
    }
   ],
   "source": [
    "finq_bench_corpus = pd.read_json(finq_bench_corpus_path, lines=True)\n",
    "finq_bench_queries = pd.read_json(finq_bench_query_path, lines=True)\n",
    "finq_bench_gt = pd.read_csv(finq_bench_tsv_path, sep='\\t')\n",
    "print(\"Dataset:FinQ Bench\\nTotal Corpus:{}\\nTotal Queries:{}\".format(finq_bench_corpus.shape[0], finq_bench_queries.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(df):\n",
    "    \"\"\"Create Documents with metadata from df\"\"\"\n",
    "    documents = []\n",
    "    for idx,row in df.iterrows():\n",
    "        _ = Document(\n",
    "            text=row['text'], \n",
    "            metadata={'_id' : row['_id'], 'title' : row['title']}\n",
    "            )\n",
    "        documents.append(_)\n",
    "    return documents\n",
    "\n",
    "# TODO: Create Custom Retriever Class after finalizing the experiment\n",
    "# https://docs.llamaindex.ai/en/stable/examples/query_engine/CustomRetrievers/\n",
    "\n",
    "\n",
    "class RetrievalAgent:\n",
    "    def __init__(self, cfg, documents):\n",
    "        self.cfg = cfg \n",
    "        self.documents = documents \n",
    "\n",
    "        self.index , self.reranker = self.initialise_retrieval_components()\n",
    "\n",
    "    def initialise_retrieval_components(self):\n",
    "        # Create FaisVectorStore to store embeddings\n",
    "        fais_index = faiss.IndexFlatL2(self.cfg.EMBED_DIMENSION)\n",
    "        vector_store = FaissVectorStore(faiss_index=fais_index)\n",
    "        print(\"Vector Store Created\")\n",
    "\n",
    "        ## Can experiment with different transformations\n",
    "        base_pipeline = IngestionPipeline(\n",
    "            # chunk_size=256, chunk_overlap=20\n",
    "            transformations=[SentenceSplitter()],\n",
    "            vector_store=vector_store,\n",
    "            documents=self.documents\n",
    "        )\n",
    "        nodes = base_pipeline.run()\n",
    "\n",
    "        # Create vector index from base nodes\n",
    "        index = VectorStoreIndex(nodes)\n",
    "        print(\"Vector Index Initialised\")\n",
    "        \n",
    "        # Create Reranker\n",
    "        reranker = SentenceTransformerRerank(\n",
    "                    model=self.cfg.RERANKER_MODEL,\n",
    "                    top_n=self.cfg.RERANKER_TOP_N\n",
    "                )\n",
    "        print(\"Reranker Initialised\")\n",
    "        return index, reranker \n",
    "\n",
    "    def retrieve_nodes(self, query_str, with_reranker=True):\n",
    "        query_bundle = QueryBundle(query_str)\n",
    "        # configure retriever\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=self.index,\n",
    "            similarity_top_k=self.cfg.SIM_TOP_K\n",
    "        )\n",
    "        retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "        if with_reranker:    \n",
    "            retrieved_nodes = self.reranker.postprocess_nodes(\n",
    "                retrieved_nodes, query_bundle\n",
    "            )\n",
    "\n",
    "        return retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_nodes(nodes, extract_unique=True):\n",
    "    init_rows = []\n",
    "    for node in nodes:\n",
    "        tmp = {\n",
    "            \"score\" : node.score,\n",
    "            \"text\" : node.text,\n",
    "            \"corpus_id\" : node.metadata['_id']\n",
    "        }\n",
    "        init_rows.append(tmp)\n",
    "    tmp_df = pd.DataFrame(init_rows)\n",
    "\n",
    "    if not extract_unique:\n",
    "        return tmp_df \n",
    "    \n",
    "    final_rows = []\n",
    "    for corpus_id, corpus_df in tmp_df.groupby('corpus_id'):\n",
    "        max_score = corpus_df['score'].max()\n",
    "        text = corpus_df[corpus_df.score == max_score].text.tolist()[0]\n",
    "        final_rows.append({\n",
    "            \"corpus_id\" : corpus_id, \n",
    "            \"text\" : text, \n",
    "            \"score\" : max_score\n",
    "        })\n",
    "    df = pd.DataFrame(final_rows)\n",
    "    df = df.sort_values(by='score', ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Created\n",
      "Vector Index Initialised\n",
      "Reranker Initialised\n"
     ]
    }
   ],
   "source": [
    "# Create FinQ Bench Documents\n",
    "finq_bench_documents = create_documents(finq_bench_corpus)\n",
    "\n",
    "# Initialize Retrieval Agent \n",
    "ret_agent = RetrievalAgent(cfg=cfg, documents=finq_bench_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = finq_bench_queries.iloc[10].text\n",
    "nodes = ret_agent.retrieve_nodes(sample_query)\n",
    "node_df = create_df_from_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id_list = []\n",
    "corpus_id_list = []\n",
    "score_list = []\n",
    "\n",
    "for idx,row in finq_bench_queries.iterrows():\n",
    "    query_id = row['_id']\n",
    "    query_text = row['text']\n",
    "\n",
    "    nodes = ret_agent.retrieve_nodes(query_text)\n",
    "    # Extract top 10 unique corpus_id\n",
    "    node_df = create_df_from_nodes(nodes)[:10]\n",
    "\n",
    "    query_id_list.extend([query_id] * 10)\n",
    "    corpus_id_list.extend(node_df.corpus_id.tolist())\n",
    "    score_list.extend(node_df.score.tolist())\n",
    "\n",
    "\n",
    "final_df = pd.DataFrame({\n",
    "    \"query_id\" : query_id_list, \n",
    "    \"corpus_id\" : corpus_id_list,\n",
    "    \"score\" : score_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'NDCG@1': 0.86667, 'NDCG@5': 0.89623, 'NDCG@10': 0.91862},\n",
       " {'MAP@1': 0.86667, 'MAP@5': 0.88444, 'MAP@10': 0.89417},\n",
       " {'Recall@1': 0.86667, 'Recall@5': 0.93333, 'Recall@10': 1.0},\n",
       " {'P@1': 0.86667, 'P@5': 0.18667, 'P@10': 0.1})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the TSV data into a dictionary format for evaluation\n",
    "qrels_dict = finq_bench_gt.groupby('query_id').apply(lambda x: dict(zip(x['corpus_id'], x['score']))).to_dict()\n",
    "results = final_df.groupby('query_id').apply(lambda x: dict(zip(x['corpus_id'], x['score']))).to_dict()\n",
    "evaluate_rag(qrels_dict, results, [1, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10 ('fin_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24178e9080b9111c74efca0fe0e59b8013241aff9e5d7780cefaf674cfabb6e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
